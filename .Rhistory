Rcpp::sourceCpp("~/Desktop/example.cpp")
simple_linear_regression(x, y)
Rcpp::sourceCpp("~/Desktop/Duke Univeristy MB Program/BIOSTAT 823/homework/hw4/optim_example.cpp")
optim_test()
Rcpp::sourceCpp("~/Desktop/example.cpp")
simple_linear_regression(x, y)
Rcpp::sourceCpp("~/Desktop/example.cpp")
simple_linear_regression(x, y)
Rcpp::sourceCpp("~/Desktop/example.cpp")
simple_linear_regression(x, y)
Rcpp::sourceCpp("~/Desktop/example.cpp")
simple_linear_regression(x, y)
Rcpp::sourceCpp("~/Desktop/example.cpp")
simple_linear_regression(x, y)
Rcpp::sourceCpp("~/Desktop/example.cpp")
simple_linear_regression(x, y)
f1 <- function(x) {
x/(x+1) * exp(-x)
}
f2 <- function(x) {
1/(x+1) * exp(-x)
}
integrate(f1, 0, 1) + integrate(f2, 1, Inf)
integrate(f1, 0, 1)
integrate(f1, 0, 1) -> int1
f1 <- function(x) {
x/(x+1) * exp(-x)
}
f2 <- function(x) {
1/(x+1) * exp(-x)
}
integrate(f1, 0, 1)$value + integrate(f2, 0, Inf)$value
f1 <- function(x) {
x/(x+1) * exp(-x)
}
f2 <- function(x) {
1/(x+1) * exp(-x)
}
integrate(f1, 0, 1)$value + integrate(f2, 1, Inf)$value
rm(int1)
X <- cbind(1, x)
X
theta <- solve(t(X)%*%X)%*%t(X)%*%y
rm(theta)
alpha_beta <- solve(t(X)%*%X)%*%t(X)%*%y
y - X%*%alpha_beta
X <- cbind(1, x)
alpha_beta <- solve(t(X)%*%X)%*%t(X)%*%y
sigmaSq <- sum((y - X%*%alpha_beta)^2)/n
c(alpha_beta, sigmaSq)
X <- cbind(1, x)
alpha_beta <- solve(t(X)%*%X)%*%t(X)%*%y
sigmaSq <- sum((y - X%*%alpha_beta)^2)/n
c(alpha_beta, sigmaSq)
var(y)
0.097^2
y/x
reticulate::repl_python()
Rcpp::sourceCpp("~/Desktop/example.cpp")
Rcpp::sourceCpp("~/Desktop/example.cpp")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
set.seed(1281)
n <- 100
x <- runif(n, 0, 1)
y <- 1 + 0.5 *x + rnorm(n, 0, 0.1)
simple_linear_regression(x, y, c(0, 0, 0.5))
simple_linear_regression(x, y, c(0, 0, 0.0256))
simple_linear_regression(x, y, c(0, 0, 0.03))
simple_linear_regression(x, y, c(0, 0, 0.01))
simple_linear_regression(x, y, c(0, 0, 0.02))
simple_linear_regression(x, y, c(0, 0, 0.025))
simple_linear_regression(x, y, c(0, 0, 0.027))
simple_linear_regression(x, y, c(0, 0, 0.028))
simple_linear_regression(x, y, c(0, 0, 0.03))
simple_linear_regression(x, y, c(0, 0, 0.031))
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
opts_current$get()$label
plot(cars)
opts_current$get()$label
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
df1 <- read.csv("./upper_level_course.csv")
df1 <- read.csv("./upper_level_course.csv")
df1 <- read.csv("./upper_level_course.csv", header = TRUE)
df1 <- read.csv("./upper_level_course.csv", header = TRUE, row.names = NULL)
df1
df1 <- read.csv("./upper_level_course.csv", header = TRUE, row.names = NULL)
df1 %>% knitr::kable(booktabs = TRUE) %>% kableExtra::kable_styling()
df1 <- read.csv("./upper_level_course.csv", header = TRUE, row.names = NULL)
df1 %>% knitr::kable(booktabs = TRUE) %>% kableExtra::kable_styling()
df1 <- read.csv("./upper_level_course.csv", header = TRUE)
df1 %>% knitr::kable(booktabs = TRUE) %>% kableExtra::kable_styling()
View(df1)
df1 <- read.csv("./upper_level_course.csv", header = TRUE)
df1 %>% knitr::kable(booktabs = TRUE) %>% kableExtra::kable_styling()
df1 <- read.csv("./upper_level_course.csv", header = TRUE)
df1$X <- NULL
df1 %>% knitr::kable(booktabs = TRUE, col.names = c("Course ID", "Name", "Textbook", "Grade")) %>% kableExtra::kable_styling()
df2 <- read.csv("./graduate_course.csv", header = TRUE)
df2$X <- NULL
df2 %>% knitr::kable(booktabs = TRUE, col.names = c("Course ID", "Name", "Textbook", "Grade")) %>% kableExtra::kable_styling()
df1 <- read.csv("./upper_level_course.csv", header = TRUE)
df1$X <- NULL
df1 %>% knitr::kable(booktabs = TRUE, col.names = c("Course ID", "Name", "Textbook", "Grade")) %>% kableExtra::kable_styling()
df2 <- read.csv("./graduate_course.csv", header = TRUE)
df2$X <- NULL
df2 %>% knitr::kable(booktabs = TRUE, col.names = c("Course ID", "Name", "Textbook", "Grade")) %>% kableExtra::kable_styling()
df2 <- read.csv("./graduate_course.csv", header = TRUE)
df2$X <- NULL
df2 %>% knitr::kable(booktabs = TRUE, col.names = c("Course ID", "Name", "Textbook", "Grade")) %>% kableExtra::kable_styling()
df2 <- read.csv("./graduate_course.csv", header = TRUE)
df2$X <- NULL
df2 %>% knitr::kable(booktabs = TRUE, col.names = c("Course ID", "Name", "Textbook", "Grade")) %>% kableExtra::kable_styling()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
set.seed(1281)
n <- 100
x <- runif(n, 0, 1)
y <- 1 + 0.5 *x + rnorm(n, 0, 0.1)
data.frame(x = x, y = y) %>%
write.csv("./prob2.csv", row.names = FALSE)
reticulate::repl_python()
knitr::opts_chunk$set(echo = TRUE)
data(iris)
str(iris)
lda.fit=lda(Species~. ,data=iris)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tidyverse)
lda.fit=lda(Species~. ,data=iris)
lda.fit
plot(lda.fit)
diabetes <- read.csv("./diabetes.csv")
rm(iris, lda.fit)
str(diabetes)
lda.fit <- lda(Outcome~. ,data=diabetes)
lda.fit
plot(lda.fit)
data("iris")
lda(Species ~ . ,data=iris)
ckd <- read.csv("./kidney_disease.csv")
str(ckd)
na.omit(ckd) %>% dim()
ckd <- read.csv("./new_model.csv")
str(ckd)
ckd %>% sapply(function(x) mean(is.na(x)))
lda.fit <- lda(Class ~ . , data = ckd)
lda.fit
plot(lda.fit)
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(ckd), replace=TRUE, prob=c(0.7, 0.3))
train <- ckd[sample, ]
test <- ckd[!sample, ]
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample, ]
test <- ckd[!sample, ]
train <- ckd[sample, ]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample), ]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
predicted <- predict(lda.model, test)
predict(lda.model)
predict(lda.model)
predict(lda.model)$x
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = Species)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = Class)
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
lda.fit <- lda(Class ~ . , data = ckd)
lda.fit
plot(lda.fit)
#scale each predictor variable
ckd[1:13] <- scale(ckd[1:13])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
predict(lda.fit)
predict(lda.fit)['x']
predict(lda.fit)
class_true <- ckd$Class
class_pred <- predict(lda.fit)["class"]
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
table(class_true, class_pred)
239 / (319)
80 / 150
seq(1,4)
ckd[-seq(1,4)]
ckd <- read.csv("./new_model.csv")
ckd <- ckd[-seq(1,4)]
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
lda.fit <- lda(Class ~ . , data = ckd)
lda.fit
plot(lda.fit)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
table(class_true, class_pred)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
conf_tbl <- table(class_true, class_pred)
conf_tbl
conf[0,0]
conf_tbl[0,0]
conf_tbl[1,1]
conf_tbl["1","1"]
conf_tbl[,"1"]
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
ckd$Rbc
ckd <- read.csv("./new_model.csv")
ckd
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn)
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
lda.fit <- lda(Class ~ . , data = ckd)
lda.fit
plot(lda.fit)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
conf_tbl <- table(class_true, class_pred)
conf_tbl
237/(13 + 237)
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
f1 <- (2 * precision * recall) / (precision + recall)
cat("precision: ", precision, "\n")
cat("recall: ", recall, "\n")
cat("F1-score: ", f1, "\n")
?caret::posPredValue
caret::posPredValue(class_pred, class_true)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
conf_tbl <- table(class_true, class_pred)
conf_tbl
caret::posPredValue(conf_tbl)
precision <- posPredValue(class_pred, class_true, positive="1")
precision <- posPredValue(class_pred, class_true, positive="1")
caret::posPredValue(class_pred, class_true, positive="1")
caret::sensitivity(class_pred, class_true, positive="1")
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
str(ckd)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
str(ckd)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
knitr::opts_chunk$set(echo = TRUE)
attach(iris)
str(iris)
lda.fit=lda(Species~. ,data=iris)
lda.fit
plot(lda.fit)
#scale each predictor variable (i.e. first 4 columns)
iris[1:4] <- scale(iris[1:4])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.8,0.2))
train <- iris[sample, ]
test <- iris[!sample, ]
#fit LDA model
lda.model <- lda(Species~., data=train)
#predict on the test set
predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = Species)
library(ggplot2)
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(LD1, LD2)) +
geom_point(aes(color = Species))
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1)) +
geom_density(aes(color = Species))
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1)) +
geom_density(aes(color = Species))
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, color = Species)) +
geom_density()
lda_plot
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
str(ckd)
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
ckd$Class <- as.factor(ckd$Class)
str(ckd)
ckd %>%
sapply(function(x) mean(is.na(x)))
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, color = Species)) +
geom_density()
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, color = Class)) +
geom_density()
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, fill = Class)) +
geom_density(alpha = 0.5)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
train.lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
train.class_true <- train$Class
train.class_pred <- predict(train.lda.model)[["class"]]
conf_tbl <- table(train.class_true, train.class_pred)
conf_tbl
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
f1 <- (2 * precision * recall) / (precision + recall)
cat("Training set: \n")
cat("precision: ", precision, "\n")
cat("recall: ", recall, "\n")
cat("F1-score: ", f1, "\n")
ldahist(test.predicted$x[,1], g = test$Class)
test.class_true <- test$Class
test.class_pred <- predict(test.lda.model)[["class"]]
test.class_true <- test$Class
test.class_pred <- test.predicted[["class"]]
conf_tbl <- table(test.class_true, test.class_pred)
conf_tbl
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
f1 <- (2 * precision * recall) / (precision + recall)
cat("Training set: \n")
cat("precision: ", precision, "\n")
cat("recall: ", recall, "\n")
cat("F1-score: ", f1, "\n")
library(glmmLasso)
data(knee)
knee[,c(2,4:6)]<-scale(knee[,c(2,4:6)],center=TRUE,scale=TRUE)
## fit adjacent category model
glm.obj <- glmmLasso(pain ~ time + th + age + sex, rnd = NULL,
family = acat(), data = knee, lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
setwd("../Desktop/Duke Univeristy MB Program/BIOSTAT 707/BIOSTAT707-Project/")
df <- read.csv("./df.csv")
df %>% str()
library(tidyverse)
df %>% str()
subset <- df[c("Mortality", "Deciduous.Forest", "Mixed.Forest")]
subset <- df[c("Mortality", "Deciduous.Forest", "Mixed.Forest")]
subset[,c(2,3)]<-scale(subset[,c(2,3)],center=TRUE,scale=TRUE)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
rm(glm.obj)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
gc()
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
subset <- df[c("Mortality", "Deciduous.Forest", "Mixed.Forest")] %>%
sample_frac(size = 0.1)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
gc()
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
subset <- df[c("Mortality", "County.FIPS", "Deciduous.Forest", "Mixed.Forest")] %>%
sample_frac(size = 0.1)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = list(County.FIPS=~1),
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
subset$County.FIPS <- as.factor(subset$County.FIPS)
df <- read.csv("./df.csv")
subset <- df[c("Mortality", "County.FIPS", "Deciduous.Forest", "Mixed.Forest")] %>%
sample_frac(size = 0.05)
subset$County.FIPS <- as.factor(subset$County.FIPS)
subset[,c(2,3)]<-scale(subset[,c(2,3)],center=TRUE,scale=TRUE)
subset[,c(3,4)]<-scale(subset[,c(3,4)],center=TRUE,scale=TRUE)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = list(County.FIPS=~1),
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)

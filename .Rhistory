#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(ckd), replace=TRUE, prob=c(0.7, 0.3))
train <- ckd[sample, ]
test <- ckd[!sample, ]
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample, ]
test <- ckd[!sample, ]
train <- ckd[sample, ]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample), ]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
predicted <- predict(lda.model, test)
predict(lda.model)
predict(lda.model)
predict(lda.model)$x
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = Species)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = Class)
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
lda.fit <- lda(Class ~ . , data = ckd)
lda.fit
plot(lda.fit)
#scale each predictor variable
ckd[1:13] <- scale(ckd[1:13])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
predict(lda.fit)
predict(lda.fit)['x']
predict(lda.fit)
class_true <- ckd$Class
class_pred <- predict(lda.fit)["class"]
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
table(class_true, class_pred)
239 / (319)
80 / 150
seq(1,4)
ckd[-seq(1,4)]
ckd <- read.csv("./new_model.csv")
ckd <- ckd[-seq(1,4)]
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
lda.fit <- lda(Class ~ . , data = ckd)
lda.fit
plot(lda.fit)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
table(class_true, class_pred)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
conf_tbl <- table(class_true, class_pred)
conf_tbl
conf[0,0]
conf_tbl[0,0]
conf_tbl[1,1]
conf_tbl["1","1"]
conf_tbl[,"1"]
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
ckd$Rbc
ckd <- read.csv("./new_model.csv")
ckd
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn)
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
lda.fit <- lda(Class ~ . , data = ckd)
lda.fit
plot(lda.fit)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
conf_tbl <- table(class_true, class_pred)
conf_tbl
237/(13 + 237)
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
f1 <- (2 * precision * recall) / (precision + recall)
cat("precision: ", precision, "\n")
cat("recall: ", recall, "\n")
cat("F1-score: ", f1, "\n")
?caret::posPredValue
caret::posPredValue(class_pred, class_true)
class_true <- ckd$Class
class_pred <- predict(lda.fit)[["class"]]
conf_tbl <- table(class_true, class_pred)
conf_tbl
caret::posPredValue(conf_tbl)
precision <- posPredValue(class_pred, class_true, positive="1")
precision <- posPredValue(class_pred, class_true, positive="1")
caret::posPredValue(class_pred, class_true, positive="1")
caret::sensitivity(class_pred, class_true, positive="1")
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
str(ckd)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
str(ckd)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
knitr::opts_chunk$set(echo = TRUE)
attach(iris)
str(iris)
lda.fit=lda(Species~. ,data=iris)
lda.fit
plot(lda.fit)
#scale each predictor variable (i.e. first 4 columns)
iris[1:4] <- scale(iris[1:4])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.8,0.2))
train <- iris[sample, ]
test <- iris[!sample, ]
#fit LDA model
lda.model <- lda(Species~., data=train)
#predict on the test set
predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = Species)
library(ggplot2)
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(LD1, LD2)) +
geom_point(aes(color = Species))
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1)) +
geom_density(aes(color = Species))
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1)) +
geom_density(aes(color = Species))
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, color = Species)) +
geom_density()
lda_plot
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
str(ckd)
ckd %>%
sapply(function(x) mean(is.na(x)))
ckd$Class <- as.factor(ckd$Class)
ckd <- read.csv("./new_model.csv")
ckd <- ckd %>%
select(-Rbc, -Htn) # removing binary variable as LDA only works for continuous data
ckd$Class <- as.factor(ckd$Class)
str(ckd)
ckd %>%
sapply(function(x) mean(is.na(x)))
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
lda.values <- predict(lda.model)
ldahist(lda.values$x[,1], g = train$Class)
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, color = Species)) +
geom_density()
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, color = Class)) +
geom_density()
# plot train dataset results
#define data to plot
lda_plot <- cbind(train, predict(lda.model)$x)
#create plot
ggplot(lda_plot, aes(x = LD1, fill = Class)) +
geom_density(alpha = 0.5)
#scale each predictor variable
ckd[1:11] <- scale(ckd[1:11])
#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(seq_len(nrow(ckd)), size = 0.7 * nrow(ckd))
train <- ckd[sample,]
test <- ckd[setdiff(seq_len(nrow(ckd)), sample),]
#fit LDA model
train.lda.model <- lda(Class ~ ., data = train)
#predict on the test set
test.predicted <- predict(lda.model, test)
train.class_true <- train$Class
train.class_pred <- predict(train.lda.model)[["class"]]
conf_tbl <- table(train.class_true, train.class_pred)
conf_tbl
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
f1 <- (2 * precision * recall) / (precision + recall)
cat("Training set: \n")
cat("precision: ", precision, "\n")
cat("recall: ", recall, "\n")
cat("F1-score: ", f1, "\n")
ldahist(test.predicted$x[,1], g = test$Class)
test.class_true <- test$Class
test.class_pred <- predict(test.lda.model)[["class"]]
test.class_true <- test$Class
test.class_pred <- test.predicted[["class"]]
conf_tbl <- table(test.class_true, test.class_pred)
conf_tbl
precision <- conf_tbl["1","1"] / sum(conf_tbl[,"1"])
recall <- conf_tbl["1","1"] / sum(conf_tbl["1",])
f1 <- (2 * precision * recall) / (precision + recall)
cat("Training set: \n")
cat("precision: ", precision, "\n")
cat("recall: ", recall, "\n")
cat("F1-score: ", f1, "\n")
library(glmmLasso)
data(knee)
knee[,c(2,4:6)]<-scale(knee[,c(2,4:6)],center=TRUE,scale=TRUE)
## fit adjacent category model
glm.obj <- glmmLasso(pain ~ time + th + age + sex, rnd = NULL,
family = acat(), data = knee, lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
setwd("../Desktop/Duke Univeristy MB Program/BIOSTAT 707/BIOSTAT707-Project/")
df <- read.csv("./df.csv")
df %>% str()
library(tidyverse)
df %>% str()
subset <- df[c("Mortality", "Deciduous.Forest", "Mixed.Forest")]
subset <- df[c("Mortality", "Deciduous.Forest", "Mixed.Forest")]
subset[,c(2,3)]<-scale(subset[,c(2,3)],center=TRUE,scale=TRUE)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
rm(glm.obj)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
gc()
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
subset <- df[c("Mortality", "Deciduous.Forest", "Mixed.Forest")] %>%
sample_frac(size = 0.1)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=FALSE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
gc()
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = NULL,
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
subset <- df[c("Mortality", "County.FIPS", "Deciduous.Forest", "Mixed.Forest")] %>%
sample_frac(size = 0.1)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = list(County.FIPS=~1),
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
subset$County.FIPS <- as.factor(subset$County.FIPS)
df <- read.csv("./df.csv")
subset <- df[c("Mortality", "County.FIPS", "Deciduous.Forest", "Mixed.Forest")] %>%
sample_frac(size = 0.05)
subset$County.FIPS <- as.factor(subset$County.FIPS)
subset[,c(2,3)]<-scale(subset[,c(2,3)],center=TRUE,scale=TRUE)
subset[,c(3,4)]<-scale(subset[,c(3,4)],center=TRUE,scale=TRUE)
## fit adjacent category model
glm.obj <- glmmLasso(Mortality ~ Deciduous.Forest + Mixed.Forest, rnd = list(County.FIPS=~1),
family = gaussian(link="identity"), data = subset,
lambda=10, final.re=TRUE,
switch.NR=FALSE, control=list(print.iter=TRUE))
summary(glm.obj)
df <- read.csv("./df.csv")
ls
df
library(tidyverse)
# drop index, FIPS, State, Year
df <- df %>% select(-X, -FIPS, -State, -Year)
df
head(df)
# drop index, FIPS, State, Year
df <- df %>% select(-X, -FIPS, -State, -Year, -County)
df <- read.csv("./df.csv")
# drop index, FIPS, State, Year
df <- df %>% select(-X, -FIPS, -State, -Year, -County)
head(df)
df_summarized <- df %>%
group_by(County.FIPS) %>%
summarise_all(mean)
df_summarized
head()
head(df)
View(df)
View(df_summarized)
colnames(df)
colnames(df_summarized)
df_summarized <- df %>%
group_by(County.FIPS) %>%
summarise_all(mean) %>%
select(-County.FIPS)
qqnorm(df_summarized$Mortality)
# select fixed effect
lasso <- cv.glmnet(
model.matrix(Mortality ~ 0 + ., data = df_summarized),
df_summarized$Mortality,
family = "gaussian",
alpha = 1,
nfolds = 10
)
library(glmnet)
# select fixed effect
lasso <- cv.glmnet(
model.matrix(Mortality ~ 0 + ., data = df_summarized),
df_summarized$Mortality,
family = "gaussian",
alpha = 1,
nfolds = 10
)
coef(lasso, s = "lambda.min")
coef(lasso, s = "lambda.1se")
model.matrix(Mortality ~ 0 + ., data = df_summarized)
model.matrix(Mortality ~ 0 + ., data = df_summarized) %>% scale()
# select fixed effect
lasso <- cv.glmnet(
scale(model.matrix(Mortality ~ 0 + ., data = df_summarized), center = TRUE, scale = TRUE),
df_summarized$Mortality,
family = "gaussian",
alpha = 1,
nfolds = 10
)
coef(lasso, s = "lambda.min")
install.packages("olsrr")
library(olsrr)
fixed_lm <- lm(Mortality ~ ., data = df_summarized)
summary(lm)
summary(fixed_lm)
lasso$glmnet.fit$dev.ratio
stepwise <- ols_step_both_p(fixed_lm, pent = 0.05, prem = 0.05)
stepwise
df
fixed_lm %>% summary()
setwd("../Desktop/Duke Univeristy MB Program/BIOSTAT 707/BIOSTAT707-Project/")
df <- read.csv("./Data/df_classes.csv")
colnames(df)
# drop index, FIPS, State, Year
df <- df %>% select(-X, -FIPS, -State, -Year, -County, -forest, -dev)
df %>% head()
# drop index, FIPS, State, Year
df <- df %>% select(-FIPS, -State, -Year, -County, -forest, -dev)
df_summarized <- df %>%
group_by(County.FIPS) %>%
summarise_all(mean) %>%
select(-County.FIPS)
head(df_summarized)
head(df_summarized) %>% head()
head(df_summarized) %>% View()
# select fixed effect
lasso <- cv.glmnet(
scale(model.matrix(Mortality ~ 0 + ., data = df_summarized), center = TRUE, scale = TRUE),
df_summarized$Mortality,
family = "gaussian",
alpha = 1,
nfolds = 10
)
lasso$glmnet.fit$dev.ratio
fixed_lm <- lm(Mortality ~ ., data = df_summarized)
summary(fixed_lm)
stepwise <- ols_step_both_p(fixed_lm, pent = 0.05, prem = 0.05)
stepwise
df$Forest.Classes..FIXED. <- as.factor(df$Forest.Classes..FIXED.)
df$Development.Classes..FIXED. <- as.factor(df$Development.Classes..FIXED.)
df_summarized <- df %>%
group_by(County.FIPS) %>%
summarise_all(mean) %>%
select(-County.FIPS)
# select fixed effect
lasso <- cv.glmnet(
scale(model.matrix(Mortality ~ 0 + ., data = df_summarized), center = TRUE, scale = TRUE),
df_summarized$Mortality,
family = "gaussian",
alpha = 1,
nfolds = 10
)
df_summarized <- df %>%
group_by(County.FIPS) %>%
summarise_all(mean) %>%
select(-County.FIPS)
df <- read.csv("./Data/df_classes.csv")
# drop index, FIPS, State, Year
df <- df %>% select(-FIPS, -State, -Year, -County, -forest, -dev)
df_summarized <- df %>%
group_by(County.FIPS) %>%
summarise_all(mean) %>%
select(-County.FIPS)
# 3104 counties
df_summarized$Forest.Classes..FIXED. <- as.factor(df_summarized$Forest.Classes..FIXED.)
df_summarized$Development.Classes..FIXED. <- as.factor(df_summarized$Development.Classes..FIXED.)
# select fixed effect
lasso <- cv.glmnet(
scale(model.matrix(Mortality ~ 0 + ., data = df_summarized), center = TRUE, scale = TRUE),
df_summarized$Mortality,
family = "gaussian",
alpha = 1,
nfolds = 10
)
lasso$glmnet.fit$dev.ratio
fixed_lm <- lm(Mortality ~ ., data = df_summarized)
summary(fixed_lm)
stepwise <- ols_step_both_p(fixed_lm, pent = 0.05, prem = 0.05)
stepwise
lm(Mortality ~ Forest.Classes..FIXED. + Development.Classes..FIXED., data = df_summarized) %>% summary()
aov(Mortality ~ Forest.Classes..FIXED. + Development.Classes..FIXED., data = df_summarized) %>% summary()
lm(Mortality ~ Forest.Classes..FIXED. + Development.Classes..FIXED., data = df_summarized) %>% summary()
aov(Mortality ~ Forest.Classes..FIXED. * Development.Classes..FIXED., data = df_summarized) %>% summary()
coef(lasso, s = "lambda.min")
coef(lasso, s = "lambda.1se")
gc()
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("./docker_concept.png")
knitr::include_graphics("./docker_concept.png")
knitr::include_graphics("./docker_concept.png")
knitr::include_graphics("./ls_screen_shot.png")
knitr::include_graphics("./ls_screen_shot.png")
